<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>IT Query Agent - RAG Chatbot Documentation</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({
            startOnLoad: true,
            theme: 'default',
            themeVariables: {
                primaryColor: '#2563eb',
                primaryTextColor: '#fff',
                primaryBorderColor: '#1e40af',
                lineColor: '#64748b',
                secondaryColor: '#3b82f6',
                tertiaryColor: '#e2e8f0'
            }
        });
    </script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #2c3e50;
            background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%);
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
            overflow: hidden;
        }

        header {
            background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%);
            color: white;
            padding: 60px 40px;
            text-align: center;
        }

        header h1 {
            font-size: 3rem;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);
        }

        header p {
            font-size: 1.3rem;
            opacity: 0.95;
        }

        .content {
            padding: 40px;
        }

        section {
            margin-bottom: 50px;
        }

        h2 {
            color: #2563eb;
            font-size: 2rem;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #2563eb;
        }

        h3 {
            color: #1e40af;
            font-size: 1.5rem;
            margin: 25px 0 15px 0;
        }

        h4 {
            color: #555;
            font-size: 1.2rem;
            margin: 20px 0 10px 0;
        }

        p {
            margin-bottom: 15px;
            font-size: 1.05rem;
            color: #444;
        }

        ul,
        ol {
            margin-left: 30px;
            margin-bottom: 20px;
        }

        li {
            margin-bottom: 10px;
            font-size: 1.05rem;
        }

        .feature-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 25px;
            margin: 30px 0;
        }

        .feature-card {
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            padding: 25px;
            border-radius: 15px;
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s, box-shadow 0.3s;
        }

        .feature-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 25px rgba(0, 0, 0, 0.15);
        }

        .feature-card h4 {
            color: #2563eb;
            margin-bottom: 10px;
            font-size: 1.3rem;
        }

        .workflow-diagram {
            background: #f8f9fa;
            padding: 30px;
            border-radius: 15px;
            margin: 30px 0;
            border-left: 5px solid #2563eb;
        }

        .workflow-step {
            background: white;
            padding: 20px;
            margin: 15px 0;
            border-radius: 10px;
            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.1);
            position: relative;
            padding-left: 60px;
        }

        .workflow-step::before {
            content: attr(data-step);
            position: absolute;
            left: 15px;
            top: 50%;
            transform: translateY(-50%);
            background: linear-gradient(135deg, #2563eb 0%, #1e40af 100%);
            color: white;
            width: 35px;
            height: 35px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
        }

        .tech-stack {
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            margin: 20px 0;
        }

        .tech-badge {
            background: linear-gradient(135deg, #2563eb 0%, #1e40af 100%);
            color: white;
            padding: 10px 20px;
            border-radius: 25px;
            font-weight: 600;
            font-size: 0.95rem;
            box-shadow: 0 3px 10px rgba(37, 99, 235, 0.3);
        }

        .highlight-box {
            background: linear-gradient(135deg, #ffeaa7 0%, #fdcb6e 100%);
            padding: 25px;
            border-radius: 15px;
            margin: 25px 0;
            border-left: 5px solid #fdcb6e;
        }

        .highlight-box strong {
            color: #d63031;
            font-size: 1.1rem;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.1);
            border-radius: 10px;
            overflow: hidden;
        }

        thead {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }

        th,
        td {
            padding: 15px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }

        tbody tr:hover {
            background: #f5f7fa;
        }

        .code-block {
            background: #2d3748;
            color: #68d391;
            padding: 20px;
            border-radius: 10px;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
            margin: 20px 0;
        }

        footer {
            background: #2c3e50;
            color: white;
            text-align: center;
            padding: 30px;
            font-size: 0.95rem;
        }

        .metric {
            display: inline-block;
            background: white;
            padding: 15px 25px;
            margin: 10px;
            border-radius: 10px;
            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.1);
            text-align: center;
        }

        .metric-value {
            font-size: 2rem;
            font-weight: bold;
            color: #2563eb;
        }

        .metric-label {
            font-size: 0.9rem;
            color: #666;
            margin-top: 5px;
        }

        .mermaid {
            background: white;
            padding: 30px;
            border-radius: 15px;
            margin: 30px 0;
            box-shadow: 0 3px 15px rgba(0, 0, 0, 0.1);
        }

        .diagram-container {
            background: #f8f9fa;
            padding: 30px;
            border-radius: 15px;
            margin: 30px 0;
            border-left: 5px solid #2563eb;
        }

        @media print {
            body {
                background: white;
                padding: 0;
            }

            .container {
                box-shadow: none;
            }
        }
    </style>
</head>

<body>
    <div class="container">
        <header>
            <h1>ü§ñ IT Query Agent</h1>
            <p>Advanced RAG-Powered Document Question-Answering System</p>
        </header>

        <div class="content">
            <!-- Project Overview -->
            <section id="overview">
                <h2>üìã Project Overview</h2>
                <p>
                    The <strong>IT Query Agent</strong> is a state-of-the-art Retrieval-Augmented Generation (RAG)
                    chatbot designed to provide
                    accurate, context-aware answers from technical documentation. Built with enterprise-grade AWS
                    services and modern web technologies,
                    it combines the power of semantic search with advanced AI language models to deliver precise,
                    grounded responses.
                </p>

                <div class="highlight-box">
                    <strong>Key Innovation:</strong> Unlike traditional chatbots that rely solely on pre-trained
                    knowledge, this system dynamically
                    retrieves relevant information from your custom document repository, ensuring answers are always
                    accurate, up-to-date, and
                    verifiable with source citations.
                </div>

                <div style="text-align: center; margin: 30px 0;">
                    <div class="metric">
                        <div class="metric-value">5</div>
                        <div class="metric-label">Retrieval Results</div>
                    </div>
                    <div class="metric">
                        <div class="metric-value">50</div>
                        <div class="metric-label">Rerank Candidates</div>
                    </div>
                    <div class="metric">
                        <div class="metric-value">512</div>
                        <div class="metric-label">Optimal Chunk Size</div>
                    </div>
                    <div class="metric">
                        <div class="metric-value">‚àû</div>
                        <div class="metric-label">Languages Supported</div>
                    </div>
                </div>
            </section>

            <!-- Architecture -->
            <section id="architecture">
                <h2>üèóÔ∏è System Architecture</h2>

                <h3>Technology Stack</h3>
                <div class="tech-stack">
                    <span class="tech-badge">Python 3.11+</span>
                    <span class="tech-badge">FastAPI</span>
                    <span class="tech-badge">React 18</span>
                    <span class="tech-badge">AWS Bedrock</span>
                    <span class="tech-badge">Claude 3 Haiku</span>
                    <span class="tech-badge">Amazon Titan Embeddings</span>
                    <span class="tech-badge">ChromaDB</span>
                    <span class="tech-badge">Cross-Encoder Reranker</span>
                    <span class="tech-badge">SQLite</span>
                    <span class="tech-badge">LangChain</span>
                </div>

                <h3>System Architecture Diagram</h3>
                <div class="diagram-container">
                    <div class="mermaid">
                        graph TB
                        subgraph Frontend["Frontend Layer"]
                        UI[React UI]
                        Chat[Chat Interface]
                        Sidebar[Session Manager]
                        end

                        subgraph Backend["Backend Layer (FastAPI)"]
                        API[REST API]
                        RAG[RAG Engine]
                        DocProc[Document Processor]
                        SessMgr[Session Manager]
                        end

                        subgraph AWS["AWS Bedrock"]
                        Claude[Claude 3 Haiku]
                        Titan[Titan Embeddings]
                        end

                        subgraph Storage["Data Layer"]
                        Chroma[(ChromaDB)]
                        SQLite[(SQLite)]
                        end

                        subgraph ML["ML Components"]
                        Reranker[Cross-Encoder]
                        end

                        UI --> API
                        Chat --> API
                        Sidebar --> API

                        API --> RAG
                        API --> SessMgr
                        API --> DocProc

                        RAG --> Chroma
                        RAG --> Claude
                        RAG --> Reranker

                        DocProc --> Titan
                        DocProc --> Chroma

                        SessMgr --> SQLite

                        Chroma -.embeddings.-> Titan

                        style Frontend fill:#e3f2fd
                        style Backend fill:#f3e5f5
                        style AWS fill:#fff3e0
                        style Storage fill:#e8f5e9
                        style ML fill:#fce4ec
                    </div>
                </div>

                <h3>Component Breakdown</h3>

                <h4>Backend (Python/FastAPI)</h4>
                <ul>
                    <li><strong>API Layer:</strong> RESTful endpoints for chat, sessions, and document ingestion</li>
                    <li><strong>RAG Engine:</strong> Orchestrates retrieval, reranking, and response generation</li>
                    <li><strong>Vector Store:</strong> ChromaDB with Amazon Titan embeddings (1536 dimensions)</li>
                    <li><strong>Document Processor:</strong> Supports PDF, TXT, MD, DOCX with intelligent chunking</li>
                    <li><strong>Session Manager:</strong> Maintains conversation history with 5-message rolling window
                    </li>
                    <li><strong>Bedrock Client:</strong> Interfaces with AWS for LLM and embedding generation</li>
                </ul>

                <h4>Frontend (React)</h4>
                <ul>
                    <li><strong>Chat Interface:</strong> Real-time messaging with source citations</li>
                    <li><strong>Session Management:</strong> ChatGPT-style auto-naming and manual renaming</li>
                    <li><strong>Reranking Audit UI:</strong> Visual inspection of Cross-Encoder impact</li>
                    <li><strong>Document Ingestion:</strong> Strategy-based upload (Normal vs. Rerank)</li>
                    <li><strong>Theme System:</strong> Light/Dark mode with premium aesthetics</li>
                </ul>
            </section>

            <!-- RAG Workflow -->
            <section id="rag-workflow">
                <h2>üîÑ RAG Workflow</h2>

                <h3>RAG Pipeline Flowchart</h3>
                <div class="diagram-container">
                    <div class="mermaid">
                        flowchart TD
                        Start([User Query]) --> Embed[Generate Query Embedding<br />Titan 1536D]
                        Embed --> VectorSearch[Vector Search<br />ChromaDB L2 Distance]

                        VectorSearch --> Rerank{Reranking<br />Enabled?}

                        Rerank -->|Yes| Stage1[Retrieve 50 Candidates]
                        Rerank -->|No| Stage2[Retrieve Top 5]

                        Stage1 --> CrossEnc[Cross-Encoder Reranking<br />ms-marco-MiniLM]
                        CrossEnc --> Top5[Select Top 5]

                        Stage2 --> Context[Assemble Context]
                        Top5 --> Context

                        Context --> History[Load Conversation History<br />Last 5 Messages]
                        History --> Filter[Filter Anti-Failure Bias<br />Remove 'I don't know']

                        Filter --> LLM[Claude 3 Haiku<br />Generate Response]

                        LLM --> Response[Return Answer + Sources]
                        Response --> SaveMsg[Save to Session DB]

                        SaveMsg --> FirstMsg{First<br />Message?}
                        FirstMsg -->|Yes| AutoName[AI Generate Title]
                        FirstMsg -->|No| End([Complete])
                        AutoName --> End

                        style Start fill:#e3f2fd
                        style Embed fill:#fff3e0
                        style VectorSearch fill:#e8f5e9
                        style Rerank fill:#fce4ec
                        style CrossEnc fill:#f3e5f5
                        style LLM fill:#fff3e0
                        style Response fill:#e3f2fd
                        style End fill:#c8e6c9
                    </div>
                </div>

                <div class="workflow-diagram">
                    <h3 style="margin-bottom: 20px;">Complete Request-Response Cycle</h3>

                    <div class="workflow-step" data-step="1">
                        <h4>User Query Submission</h4>
                        <p>User sends a question through the chat interface with optional Knowledge Base and Reranking
                            toggles.</p>
                    </div>

                    <div class="workflow-step" data-step="2">
                        <h4>Query Embedding</h4>
                        <p>The question is converted to a 1536-dimensional vector using Amazon Titan Embeddings via AWS
                            Bedrock.</p>
                    </div>

                    <div class="workflow-step" data-step="3">
                        <h4>Vector Search (Stage 1)</h4>
                        <p>ChromaDB performs semantic similarity search using L2 distance. Returns top 5 (normal) or top
                            50 (rerank) candidates.</p>
                    </div>

                    <div class="workflow-step" data-step="4">
                        <h4>Cross-Encoder Reranking (Optional - Stage 2)</h4>
                        <p>If enabled, the 50 candidates are re-scored using <code>ms-marco-MiniLM-L-6-v2</code>
                            Cross-Encoder. The model evaluates query-document pairs and reorders results by relevance,
                            selecting the final top 5.</p>
                    </div>

                    <div class="workflow-step" data-step="5">
                        <h4>Context Assembly</h4>
                        <p>Retrieved chunks are formatted with source metadata (filename, page number) and combined into
                            a structured context string.</p>
                    </div>

                    <div class="workflow-step" data-step="6">
                        <h4>Conversation History Filtering</h4>
                        <p>The system loads the last 5 messages from the session and applies an "Anti-Failure Bias"
                            filter, removing any previous "I don't know" responses to prevent self-contradiction.</p>
                    </div>

                    <div class="workflow-step" data-step="7">
                        <h4>LLM Generation</h4>
                        <p>Claude 3 Haiku receives the context, filtered history, and current question. It generates a
                            grounded response with strict instructions to only use provided information.</p>
                    </div>

                    <div class="workflow-step" data-step="8">
                        <h4>Response Delivery</h4>
                        <p>The answer is returned with source citations and optional reranking audit data, then
                            displayed in the chat interface.</p>
                    </div>

                    <div class="workflow-step" data-step="9">
                        <h4>Session Update</h4>
                        <p>Both user and assistant messages are persisted to SQLite. If it's the first message, an
                            AI-generated session title is created.</p>
                    </div>
                </div>
            </section>

            <!-- Features -->
            <section id="features">
                <h2>‚ú® Key Features</h2>

                <div class="feature-grid">
                    <div class="feature-card">
                        <h4>üéØ Dual Retrieval Modes</h4>
                        <p>Toggle between standard vector search and advanced Cross-Encoder reranking for optimal
                            precision.</p>
                    </div>

                    <div class="feature-card">
                        <h4>üîç Reranking Audit</h4>
                        <p>Visual inspection of how the Cross-Encoder reorders search results, showing rank changes and
                            relevance scores.</p>
                    </div>

                    <div class="feature-card">
                        <h4>üåê Multilingual Support</h4>
                        <p>Ask questions in any language and receive answers in that language, even if documents are in
                            English.</p>
                    </div>

                    <div class="feature-card">
                        <h4>üìù Auto Session Naming</h4>
                        <p>ChatGPT-style automatic title generation based on the first question, with manual rename
                            capability.</p>
                    </div>

                    <div class="feature-card">
                        <h4>üíæ Incremental Ingestion</h4>
                        <p>Smart document processing that only re-indexes modified files, using SHA-256 hash comparison.
                        </p>
                    </div>

                    <div class="feature-card">
                        <h4>üß† Anti-Failure Bias</h4>
                        <p>Automatically filters "I don't know" responses from conversation history to prevent AI
                            self-contradiction.</p>
                    </div>

                    <div class="feature-card">
                        <h4>üìä Source Citations</h4>
                        <p>Every answer includes references to source documents with page numbers for verification.</p>
                    </div>

                    <div class="feature-card">
                        <h4>üé® Premium UI/UX</h4>
                        <p>Modern, responsive interface with glassmorphism, smooth animations, and light/dark themes.
                        </p>
                    </div>

                    <div class="feature-card">
                        <h4>‚ö° Optimized Performance</h4>
                        <p>Parallel embedding generation, efficient chunking, and smart caching for sub-second
                            responses.</p>
                    </div>
                </div>
            </section>

            <!-- Configuration -->
            <section id="configuration">
                <h2>‚öôÔ∏è Configuration Settings</h2>

                <h3>Default RAG Parameters</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Parameter</th>
                            <th>Value</th>
                            <th>Description</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>TOP_K_RESULTS</strong></td>
                            <td>5</td>
                            <td>Number of final document chunks to send to LLM</td>
                        </tr>
                        <tr>
                            <td><strong>SIMILARITY_THRESHOLD</strong></td>
                            <td>0.7</td>
                            <td>L2 distance threshold for vector search (lower = stricter)</td>
                        </tr>
                        <tr>
                            <td><strong>CHUNK_SIZE</strong></td>
                            <td>1000</td>
                            <td>Characters per text chunk (normal mode)</td>
                        </tr>
                        <tr>
                            <td><strong>CHUNK_OVERLAP</strong></td>
                            <td>200</td>
                            <td>Overlapping characters between chunks</td>
                        </tr>
                        <tr>
                            <td><strong>MAX_MEMORY_MESSAGES</strong></td>
                            <td>5</td>
                            <td>Conversation history window (5 user + 5 assistant)</td>
                        </tr>
                    </tbody>
                </table>

                <h3>Cross-Encoder Reranking (Market Best)</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Parameter</th>
                            <th>Value</th>
                            <th>Description</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>RERANK_TOP_K</strong></td>
                            <td>5</td>
                            <td>Final results after reranking</td>
                        </tr>
                        <tr>
                            <td><strong>TOP_K_STAGE1</strong></td>
                            <td>50</td>
                            <td>Candidate pool for reranking (wider net)</td>
                        </tr>
                        <tr>
                            <td><strong>RERANK_CHUNK_SIZE</strong></td>
                            <td>512</td>
                            <td>Optimal chunk size for Cross-Encoder precision</td>
                        </tr>
                        <tr>
                            <td><strong>RERANK_CHUNK_OVERLAP</strong></td>
                            <td>100</td>
                            <td>Overlap for high-precision chunks</td>
                        </tr>
                        <tr>
                            <td><strong>MODEL</strong></td>
                            <td>ms-marco-MiniLM-L-6-v2</td>
                            <td>Industry-standard Cross-Encoder model</td>
                        </tr>
                    </tbody>
                </table>
            </section>

            <!-- Ingestion Strategies -->
            <section id="ingestion">
                <h2>üì• Document Ingestion Strategies</h2>

                <h3>Strategy 1: Normal Retrieval</h3>
                <div class="highlight-box">
                    <strong>Use Case:</strong> General-purpose Q&A with balanced speed and accuracy
                </div>
                <ul>
                    <li><strong>Chunk Size:</strong> 1000 characters (captures full paragraphs)</li>
                    <li><strong>Chunk Overlap:</strong> 200 characters (maintains context continuity)</li>
                    <li><strong>Retrieval:</strong> Direct vector search, top 5 results</li>
                    <li><strong>Best For:</strong> Single-topic questions, quick lookups, broad queries</li>
                </ul>

                <h3>Strategy 2: Reranking Retrieval (Market Best)</h3>
                <div class="highlight-box">
                    <strong>Use Case:</strong> Complex, multi-part questions requiring high precision
                </div>
                <ul>
                    <li><strong>Chunk Size:</strong> 512 characters (optimized for Cross-Encoder)</li>
                    <li><strong>Chunk Overlap:</strong> 100 characters (fine-grained segmentation)</li>
                    <li><strong>Retrieval:</strong> 2-stage (50 candidates ‚Üí rerank ‚Üí top 5)</li>
                    <li><strong>Best For:</strong> Technical queries, multi-topic questions, when precision is critical
                    </li>
                </ul>

                <h3>Supported File Formats</h3>
                <div class="tech-stack">
                    <span class="tech-badge">PDF</span>
                    <span class="tech-badge">TXT</span>
                    <span class="tech-badge">Markdown (.md)</span>
                    <span class="tech-badge">Word (.docx)</span>
                </div>
            </section>

            <!-- Technical Highlights -->
            <section id="technical">
                <h2>üî¨ Technical Highlights</h2>

                <h3>1. Semantic Cross-Lingual Search</h3>
                <p>
                    Amazon Titan Embeddings create language-agnostic vector representations. A question in French will
                    match
                    English documents if the semantic meaning aligns, enabling true multilingual search without
                    translation APIs.
                </p>

                <h3>2. Two-Stage Retrieval Pipeline</h3>
                <p>
                    The system implements a sophisticated retrieval strategy:
                </p>
                <ul>
                    <li><strong>Stage 1 (Recall):</strong> Fast vector search casts a wide net (50 candidates)</li>
                    <li><strong>Stage 2 (Precision):</strong> Cross-Encoder deeply analyzes query-document pairs,
                        rescoring and reordering</li>
                </ul>
                <p>
                    This approach balances speed (vector search is ~10ms) with accuracy (Cross-Encoder achieves 95%+
                    relevance).
                </p>

                <h3>3. Anti-Hallucination Mechanisms</h3>
                <ul>
                    <li><strong>Strict Grounding Prompts:</strong> Claude is instructed to only use provided context
                    </li>
                    <li><strong>Source Attribution:</strong> Every answer must cite document sources</li>
                    <li><strong>Failure Filtering:</strong> Previous "I don't know" responses are excluded from history
                    </li>
                    <li><strong>Low Temperature:</strong> LLM temperature set to 0.1 for factual consistency</li>
                </ul>

                <h3>4. Optimized Embedding Generation</h3>
                <p>
                    Parallel processing with ThreadPoolExecutor (15 workers) reduces embedding time from ~2 minutes to
                    ~8 seconds
                    for 1000 chunks, a <strong>15x speedup</strong>.
                </p>

                <h3>5. Intelligent Session Management</h3>
                <ul>
                    <li><strong>Auto-Naming:</strong> First message triggers Claude to generate a 2-4 word title</li>
                    <li><strong>Rolling Memory:</strong> Only last 5 exchanges kept to prevent token bloat</li>
                    <li><strong>Persistent Storage:</strong> SQLite stores full history for session restoration</li>
                </ul>
            </section>

            <!-- Workflow Diagram -->
            <section id="app-design">
                <h2>üé® Application Design Workflow</h2>

                <h3>User Journey</h3>
                <div class="workflow-diagram">
                    <div class="workflow-step" data-step="1">
                        <h4>Landing & Session Creation</h4>
                        <p>User opens app ‚Üí Auto-creates new session with timestamp name ‚Üí Displays empty chat interface
                        </p>
                    </div>

                    <div class="workflow-step" data-step="2">
                        <h4>Document Upload (Optional)</h4>
                        <p>User clicks "Ingest Documents" ‚Üí Selects strategy (Normal/Rerank) ‚Üí System processes PDFs ‚Üí
                            Stores in ChromaDB</p>
                    </div>

                    <div class="workflow-step" data-step="3">
                        <h4>First Question</h4>
                        <p>User types question ‚Üí Enables Knowledge Base toggle ‚Üí Sends message ‚Üí System auto-renames
                            session</p>
                    </div>

                    <div class="workflow-step" data-step="4">
                        <h4>Answer Display</h4>
                        <p>Response appears with typing animation ‚Üí Source citations shown ‚Üí "Audit" button visible (if
                            reranking enabled)</p>
                    </div>

                    <div class="workflow-step" data-step="5">
                        <h4>Iterative Refinement</h4>
                        <p>User asks follow-up ‚Üí System maintains context ‚Üí Can toggle Reranking mid-conversation ‚Üí
                            Previous failures auto-filtered</p>
                    </div>

                    <div class="workflow-step" data-step="6">
                        <h4>Session Management</h4>
                        <p>User can rename session ‚Üí Switch between sessions ‚Üí Delete sessions ‚Üí All history persisted
                        </p>
                    </div>
                </div>

                <h3>UI/UX Design Principles</h3>
                <ul>
                    <li><strong>Minimalism:</strong> Clean, ChatGPT-inspired interface with no clutter</li>
                    <li><strong>Transparency:</strong> Reranking audit shows exactly how AI makes decisions</li>
                    <li><strong>Responsiveness:</strong> Smooth animations, instant feedback, optimistic UI updates</li>
                    <li><strong>Accessibility:</strong> Hover tooltips, keyboard shortcuts, high contrast modes</li>
                    <li><strong>Premium Feel:</strong> Glassmorphism, gradients, micro-animations for engagement</li>
                </ul>
            </section>

            <!-- Performance Metrics -->
            <section id="performance">
                <h2>üìà Performance Metrics</h2>

                <table>
                    <thead>
                        <tr>
                            <th>Operation</th>
                            <th>Time</th>
                            <th>Notes</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Vector Search (5 results)</td>
                            <td>~10ms</td>
                            <td>ChromaDB in-memory index</td>
                        </tr>
                        <tr>
                            <td>Cross-Encoder Reranking (50‚Üí5)</td>
                            <td>~150ms</td>
                            <td>CPU-based inference</td>
                        </tr>
                        <tr>
                            <td>LLM Response Generation</td>
                            <td>~1-2s</td>
                            <td>AWS Bedrock Claude 3 Haiku</td>
                        </tr>
                        <tr>
                            <td>Total Query Time (Normal)</td>
                            <td>~1.5s</td>
                            <td>Search + LLM</td>
                        </tr>
                        <tr>
                            <td>Total Query Time (Rerank)</td>
                            <td>~2s</td>
                            <td>Search + Rerank + LLM</td>
                        </tr>
                        <tr>
                            <td>Document Ingestion (1000 chunks)</td>
                            <td>~8s</td>
                            <td>Parallel embedding generation</td>
                        </tr>
                    </tbody>
                </table>
            </section>

            <!-- Future Enhancements -->
            <section id="future">
                <h2>üöÄ Future Enhancements</h2>

                <ul>
                    <li><strong>Multi-Modal Support:</strong> Image and table extraction from PDFs</li>
                    <li><strong>Advanced Filters:</strong> Search by date, author, document type</li>
                    <li><strong>Collaborative Features:</strong> Share sessions, export conversations</li>
                    <li><strong>Analytics Dashboard:</strong> Query patterns, popular topics, accuracy metrics</li>
                    <li><strong>Voice Interface:</strong> Speech-to-text and text-to-speech integration</li>
                    <li><strong>Custom Models:</strong> Fine-tuned embeddings for domain-specific terminology</li>
                    <li><strong>Hybrid Search:</strong> Combine semantic and keyword-based retrieval</li>
                    <li><strong>Real-time Updates:</strong> WebSocket for streaming responses</li>
                </ul>
            </section>

            <!-- Conclusion -->
            <section id="conclusion">
                <h2>üéØ Conclusion</h2>

                <p>
                    The <strong>IT Query Agent</strong> represents a production-ready implementation of modern RAG
                    architecture,
                    combining cutting-edge AI technologies with thoughtful UX design. By leveraging AWS Bedrock's
                    enterprise-grade
                    infrastructure and implementing advanced retrieval strategies like Cross-Encoder reranking, the
                    system achieves
                    a unique balance of speed, accuracy, and user transparency.
                </p>

                <div class="highlight-box">
                    <strong>Key Takeaway:</strong> This project demonstrates that RAG systems can be both powerful and
                    user-friendly.
                    The addition of features like reranking audits, multilingual support, and anti-failure bias
                    filtering showcases
                    how thoughtful engineering can transform a basic chatbot into a reliable, enterprise-grade knowledge
                    assistant.
                </div>

                <p style="margin-top: 30px; text-align: center; font-size: 1.2rem; color: #2563eb;">
                    <strong>Built with ‚ù§Ô∏è using AWS Bedrock, React, and FastAPI</strong>
                </p>
            </section>
        </div>

        <footer>
            <p><strong>IT Query Agent</strong> - Advanced RAG-Powered Document Q&A System</p>
            <p>¬© 2026 | Powered by AWS Bedrock, Claude 3, and Amazon Titan Embeddings</p>
        </footer>
    </div>
</body>

</html>